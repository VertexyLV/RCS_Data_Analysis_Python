{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Web Scraping ?\n",
    "\n",
    "Web scraping is a term for various methods used to collect information from across the Internet. Generally, this is done with software that simulates human Web surfing to collect specified bits of information from different websites. Via [Technopedia](https://www.techopedia.com/definition/5212/web-scraping)\n",
    "\n",
    "Converting HTML (https://developer.mozilla.org/en-US/docs/Web/HTML) into a different structure \n",
    "\n",
    "## Various methods of web scraping with Python\n",
    "\n",
    "\n",
    "* Requests\n",
    "* Beautiful Soup\n",
    "* Pandas\n",
    "* Selenium\n",
    "\n",
    "![Web Scraping](https://covers.oreillystatic.com/images/0636920078067/cat.gif)\n",
    "\n",
    "Suggested Book for learning more: http://shop.oreilly.com/product/0636920078067.do \n",
    "(using their notebooks from https://github.com/REMitchell/python-scraping as starting point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>A Useful Page</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>An Interesting Title</h1>\n",
      "<div>\n",
      "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req = requests.get('http://pythonscraping.com/pages/page1.html')\n",
    "print(req.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "# Some uses urlopen instead of requests, but requests is better in my opinion.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASCII_SPACES',\n",
       " 'DEFAULT_BUILDER_FEATURES',\n",
       " 'HTML_FORMATTERS',\n",
       " 'NO_PARSER_SPECIFIED_WARNING',\n",
       " 'ROOT_TAG_NAME',\n",
       " 'XML_FORMATTERS',\n",
       " '__bool__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_all_strings',\n",
       " '_attr_value_as_string',\n",
       " '_attribute_checker',\n",
       " '_check_markup_is_url',\n",
       " '_feed',\n",
       " '_find_all',\n",
       " '_find_one',\n",
       " '_formatter_for_name',\n",
       " '_is_xml',\n",
       " '_lastRecursiveChild',\n",
       " '_last_descendant',\n",
       " '_most_recent_element',\n",
       " '_popToTag',\n",
       " '_select_debug',\n",
       " '_selector_combinators',\n",
       " '_should_pretty_print',\n",
       " '_tag_name_matches_and',\n",
       " 'append',\n",
       " 'attribselect_re',\n",
       " 'attrs',\n",
       " 'builder',\n",
       " 'can_be_empty_element',\n",
       " 'childGenerator',\n",
       " 'children',\n",
       " 'clear',\n",
       " 'contains_replacement_characters',\n",
       " 'contents',\n",
       " 'currentTag',\n",
       " 'current_data',\n",
       " 'declared_html_encoding',\n",
       " 'decode',\n",
       " 'decode_contents',\n",
       " 'decompose',\n",
       " 'descendants',\n",
       " 'encode',\n",
       " 'encode_contents',\n",
       " 'endData',\n",
       " 'extract',\n",
       " 'fetchNextSiblings',\n",
       " 'fetchParents',\n",
       " 'fetchPrevious',\n",
       " 'fetchPreviousSiblings',\n",
       " 'find',\n",
       " 'findAll',\n",
       " 'findAllNext',\n",
       " 'findAllPrevious',\n",
       " 'findChild',\n",
       " 'findChildren',\n",
       " 'findNext',\n",
       " 'findNextSibling',\n",
       " 'findNextSiblings',\n",
       " 'findParent',\n",
       " 'findParents',\n",
       " 'findPrevious',\n",
       " 'findPreviousSibling',\n",
       " 'findPreviousSiblings',\n",
       " 'find_all',\n",
       " 'find_all_next',\n",
       " 'find_all_previous',\n",
       " 'find_next',\n",
       " 'find_next_sibling',\n",
       " 'find_next_siblings',\n",
       " 'find_parent',\n",
       " 'find_parents',\n",
       " 'find_previous',\n",
       " 'find_previous_sibling',\n",
       " 'find_previous_siblings',\n",
       " 'format_string',\n",
       " 'get',\n",
       " 'getText',\n",
       " 'get_attribute_list',\n",
       " 'get_text',\n",
       " 'handle_data',\n",
       " 'handle_endtag',\n",
       " 'handle_starttag',\n",
       " 'has_attr',\n",
       " 'has_key',\n",
       " 'hidden',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'insert_after',\n",
       " 'insert_before',\n",
       " 'isSelfClosing',\n",
       " 'is_empty_element',\n",
       " 'is_xml',\n",
       " 'known_xml',\n",
       " 'markup',\n",
       " 'name',\n",
       " 'namespace',\n",
       " 'new_string',\n",
       " 'new_tag',\n",
       " 'next',\n",
       " 'nextGenerator',\n",
       " 'nextSibling',\n",
       " 'nextSiblingGenerator',\n",
       " 'next_element',\n",
       " 'next_elements',\n",
       " 'next_sibling',\n",
       " 'next_siblings',\n",
       " 'object_was_parsed',\n",
       " 'original_encoding',\n",
       " 'parent',\n",
       " 'parentGenerator',\n",
       " 'parents',\n",
       " 'parse_only',\n",
       " 'parserClass',\n",
       " 'parser_class',\n",
       " 'popTag',\n",
       " 'prefix',\n",
       " 'preserve_whitespace_tag_stack',\n",
       " 'preserve_whitespace_tags',\n",
       " 'prettify',\n",
       " 'previous',\n",
       " 'previousGenerator',\n",
       " 'previousSibling',\n",
       " 'previousSiblingGenerator',\n",
       " 'previous_element',\n",
       " 'previous_elements',\n",
       " 'previous_sibling',\n",
       " 'previous_siblings',\n",
       " 'pushTag',\n",
       " 'quoted_colon',\n",
       " 'recursiveChildGenerator',\n",
       " 'renderContents',\n",
       " 'replaceWith',\n",
       " 'replaceWithChildren',\n",
       " 'replace_with',\n",
       " 'replace_with_children',\n",
       " 'reset',\n",
       " 'select',\n",
       " 'select_one',\n",
       " 'setup',\n",
       " 'string',\n",
       " 'strings',\n",
       " 'stripped_strings',\n",
       " 'tagStack',\n",
       " 'tag_name_re',\n",
       " 'text',\n",
       " 'unwrap',\n",
       " 'wrap']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen(\"https://pythonscrapingthisurldoesnotexist.com\")\n",
    "except HTTPError as e:\n",
    "    print(\"The server returned an HTTP error\")\n",
    "except URLError as e:\n",
    "    print(\"The server could not be found!\")\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "        title = bsObj.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "    print(\"Title could not be found\")\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
